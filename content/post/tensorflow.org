---
title: "Building tensorflow from source"
date: 2018-04-29T13:23:12+02:00
draft: false
---

* Build tensorflow-gpu on GNU/Linux Debian

  I am running GNU/Linux Debian with Nvidia 9.1 drivers and cuDNN 7.1.

  - Tensorflow-gpu pip packages are build for specific CUDA
  - Cuda files from Debian packages are placed in different location than from usual installer
    
  That means that on GNU/Linux Debian, I can not use *pip* packages directly, since versions of libraries might not match. I can not also easily compile tensorflow source against CUDA Toolkit from packages, because of misplaced header files and shared objects.
  
  I solve it by installing CUDA that matches version from packages using local insteller binary downloaded from Nvidia website. Only toolkit is required without drivers or examples. Than I build Python 3.6 package myself.
  
* Instructions
** Download tensorflow
   #+BEGIN_SRC bash
     git clone https://github.com/tensorflow/tensorflow
     cd tensorflow
     git checkout v1.8.0 # Check out latest branch
     # Patch to compile with gcc-6, at this moment this is latest supported compiler
     git cherry-pick e489b60
   #+END_SRC
   
** Download CUDA Toolkit
   Download version of [[https://developer.nvidia.com/cuda-downloads][CUDA]] that matches version from packages, in my case it is 9.1.
   Install files, except for drivers and samples, into standard location.
   
** Download CuDNN 
   Download and install [[https://developer.nvidia.com/rdp/cudnn-download][CUDA Deep neural network library]] matching CUDA Tookit
   
   #+BEGIN_EXAMPLE
   cuDNN v7.1.3 Runtime Library for Ubuntu16.04 (Deb)
   cuDNN v7.1.3 Developer Library for Ubuntu16.04 (Deb)
   #+END_EXAMPLE
   
** Download bazel
   To compile and build
   #+BEGIN_SRC bash
     # Add repository
     echo "deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8" | sudo tee /etc/apt/sources.list.d/bazel.list
     # Add key
     curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add -
     # Update packages information
     apt-get update
     # Install bazel
     apt-get install bazel
   #+END_SRC
** Compile tensorflow-gpu
   Run *configure* script and give default ansers to most questions except CUDA which should be enabled. Pick gcc compiler as gcc-6. If you don't have gcc-6 installed

   #+BEGIN_SRC bash
     apt-get install gcc-6 g++-6
   #+END_SRC
   
   Run bazel
   
   #+BEGIN_SRC bash
       bazel build --config=opt --config=cuda --incompatible_load_argument_is_label=false //tensorflow/tools/pip_package:build_pip_packagec
   #+END_SRC
   

   Building binaries took roughly 2 hours on 8 cores i7-6700HQ CPU @ 2.60GHz. Next build and install the package
   
   #+BEGIN_SRC bash
       bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
       cp /tmp/tensorflow_pkg/tensorflow-1.8.0-cp36-cp36m-linux_x86_64.whl ~/
       pip3 install ~/tensorflow-1.8.0-cp36-cp36m-linux_x86_64.whl
   #+END_SRC
** Run example
#+BEGIN_EXAMPLE
In [2]: model = Sequential()
   ...: model.add(Dense(32, activation='relu', input_dim=100))
   ...: model.add(Dense(1, activation='sigmoid'))
   ...: model.compile(optimizer='rmsprop',
   ...:               loss='binary_crossentropy',
   ...:               metrics=['accuracy'])
   ...: 
   ...: # Generate dummy data
   ...: import numpy as np
   ...: data = np.random.random((1000, 100))
   ...: labels = np.random.randint(2, size=(1000, 1))
   ...: 
   ...: # Train the model, iterating on the data in batches of 32 samples
   ...: model.fit(data, labels, epochs=10, batch_size=32)
   ...: 
   ...: 
Epoch 1/10
2018-04-29 13:03:10.454166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-04-29 13:03:10.454636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.176
pciBusID: 0000:01:00.0
totalMemory: 3.95GiB freeMemory: 3.91GiB
2018-04-29 13:03:10.454652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Ignoring visible gpu device (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0) with Cuda compute capability 5.0. The minimum required Cuda capability is 5.2.
2018-04-29 13:03:10.454660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-04-29 13:03:10.454664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2018-04-29 13:03:10.454669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
1000/1000 [==============================] - 0s 293us/step - loss: 0.7168 - acc: 0.4890
Epoch 2/10
1000/1000 [==============================] - 0s 31us/step - loss: 0.7045 - acc: 0.4940
Epoch 3/10
1000/1000 [==============================] - 0s 26us/step - loss: 0.6993 - acc: 0.5100
Epoch 4/10
1000/1000 [==============================] - 0s 27us/step - loss: 0.6937 - acc: 0.5180
Epoch 5/10
1000/1000 [==============================] - 0s 28us/step - loss: 0.6886 - acc: 0.5410
Epoch 6/10
1000/1000 [==============================] - 0s 27us/step - loss: 0.6806 - acc: 0.5560
Epoch 7/10
1000/1000 [==============================] - 0s 26us/step - loss: 0.6814 - acc: 0.5600
Epoch 8/10
1000/1000 [==============================] - 0s 28us/step - loss: 0.6736 - acc: 0.5630
Epoch 9/10
1000/1000 [==============================] - 0s 26us/step - loss: 0.6717 - acc: 0.6010
Epoch 10/10
1000/1000 [==============================] - 0s 27us/step - loss: 0.6698 - acc: 0.5990
Out[2]: <keras.callbacks.History at 0x7fd1ef63eeb8>
#+END_EXAMPLE

It works now!

